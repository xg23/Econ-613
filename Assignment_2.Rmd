---
title: "Assignment 2"
author: "Xiaoshu Gui"
output: pdf_document
---

## Setup
```{r message=FALSE, warning=FALSE}
# Load packages
library(dplyr)
library(purrr)
library(magrittr)
library(tidyr)
library(tibble)
library(stringr)
library(reshape2)
library(lme4)
```

## Exercise 1 Data Creation
```{r}
set.seed(12345)
X1 <- runif(10000, min = 1, max = 3)
X2 <- rgamma(10000, shape = 3, scale = 2)
X3 <- rbinom(10000, 1, 0.3)
esp <- rnorm(10000, 2, 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + esp
X = as.matrix(cbind(1, X1, X2, X3))
ydum <- ifelse(Y > mean(Y), 1, 0)
```

## Exercise 3 Numerical Optimization
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Likelihood function:
likelihood <- function(beta) {
  prod(pnorm(X %*% beta)^ydum * (1 - pnorm(X %*% beta)^(1 - ydum)))
}
# Log likelihood funcition:
ll <- function(beta) { 
  sum(ydum * log(pnorm(X %*% beta)) + (1 - ydum)*log(1 - pnorm(X %*% beta)))
} 
# gradient:
gradient <- function(beta) {
  u = X %*% beta
  Phi = pnorm(u)
  phi = dnorm(u)
  v = ifelse(ydum == 1, phi/Phi, -phi/(1 - Phi))
  g = t(v) %*% X
  g = t(g)
  return(g)
}
# set up for gradient ascent
beta_old = matrix(c(0.1, 0.1, 0.1, 0.1), nrow = 4, ncol = 1)
L = ll(beta_old) # an array that stores log-likelihood for each iteration; first element is the initial log-likelihood
iter = 0 # count interation
tol = 10^(-2)
step_size = 10^(-5) # scaling parameter
# gradiant ascent
while(TRUE) {
  grad = gradient(beta_old)   # compute gradient as the direction 
  beta_new = beta_old + grad * step_size # update
  L = c(L, ll(beta_new))  # store results
  diff = abs(L[length(L)] - L[length(L) - 1])
  iter = iter + 1
  beta_old = beta_new
  # stopping criteria
  if(iter == 1000){
    message("reach max iterations")
    break()
  }
  if(diff < tol){
    message("log-likelihood has converged")
    break()
  }
}
# results
#plot(L, type = "l")
L[length(L)] # log-likelihood
beta_old # coefficient
# They are quite close to the true parameters, which are 1.2, -0.9, 0.1.
```

## Exercise 2 OLS
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
cor(Y, X1)
cor.test(Y, X1, alt = "greater")
t.test(Y, X1, mu = 1.2, paired = T) # because Y is a function of X1, they are dependent. 
m1 <- lm(Y~ (1 + X1 + X2 + X3))
coef(m1)
# by hand
y = as.matrix(Y)
Beta = solve(t(X) %*% X) %*% t(X) %*% y # The restuls are the same with coef(m1)
# calculate the standard error using standand fomulas from OLS using the varcov matrix
k = ncol(X)
n = nrow(X)
# create means of each column:
X_mean <- matrix(1, nrow = n) %*% cbind(mean(X[,1]), mean(X[,2]), mean(X[,3]), mean(X[,4]))
# create a difference matrix:
D <- X - X_mean
# create the variance-covariance matrix
C <- (n-1)^(-1) * t(D) %*% D
se <- sqrt(diag(C))/sqrt(n)
# using bootstrap with 49 and 499 replications
df = data.frame(X1, X2, X3, Y)
n = nrow(df)
coef_table = matrix(0, nrow = 499, ncol = 4)
for (B in 1:499) {
  # Bootstrap sampling
  boots_index = sample(1:n, n, replace = TRUE)
  df_boots = df[boots_index, ]
  X = as.matrix(cbind(1, df_boots[, 1:(ncol(df_boots) - 1)]))
  y = as.matrix(df_boots[,ncol(df_boots)])
  # compute coefficient, store result
  coef_table[B, ] = solve(t(X)%*% X) %*% t(X) %*% y
}
# se. with 49 reps
apply(coef_table[1:49,], 2, sd)/sqrt(49)
# se. with 499 reps
apply(coef_table, 2, sd)/sqrt(499)
```

## Exercise 4
The log likelihood functions of probit, logit and linear prbability model are the same, dispite different function F(x). The probit model specifies cdf function, the logit model specifies the logistic funtion and the linear probability model specifies a linear function ($\beta$X). 
```{r}
# probit model
prob_lik <- function(beta) {  # likelihood 
  logl <- sum(ydum* log(pnorm(X %*% beta)) + (1 - ydum)* log(1 - pnorm(X %*% beta)))
  return(-logl)
} 


# the gradiant function is the same in ex.3

# logit model
logit_lik <- function(beta) {  # likelihood
  logl <- sum(ydum*(log(1 + e^(X %*% beta))^(-1)) + (1 -ydum)*log(1 - (1 + e^(X %*% beta))^(-1)))
  return(-logl)
}

# linear probability model
ll_lm <- function(beta) {
  sum(ydum*(log(X %*% beta) + (1 - ydum)*log(1 - X %*% beta)))
}

## compare with glm and lm estimtaes
m2 <- glm(ydum ~ X, family = binomial(link = "probit"))
m2$coefficients

m3 <- glm(ydum ~ X, family = "binomial")
m3$coefficients

m4 <- lm(ydum ~ X)
m4$coefficients
```


## Exercise 5
```{r}



```

